<html style=font-family:arial><head><title>IdentifySecondaryObjects</title></head><body><h1>Module: IdentifySecondaryObjects</h1><div><b>Identify Secondary Objects</b> identifies objects (e.g., cell edges) using  objects identified by another module (e.g., nuclei) as a starting point. <hr> <h4>What is a secondary object?</h4> In CellProfiler, we use the term <i>object</i> as a generic term to refer to an identifed feature in an image, usually a cellular subcompartment of some kind (for example, nuclei, cells, colonies, worms). We define an object as <i>secondary</i> when it can be  found in an image by using another cellular feature as a reference for guiding detection. <p><p>For densely-packed cells (such as those in a confluent monolayer), determining the cell borders using a cell body stain can be quite difficult since they often have irregular intensity patterns and are lower-contrast  with more diffuse staining. In addition, cells often touch their neighbors making it harder to  delineate the cell borders. It is often easier to identify an organelle which is well separated  spatially (such as the nucleus) as an object first and then use that object to guide the detection  of the cell borders. See the <b>IdentifyPrimaryObjects</b> module for details on how to identify  a primary object.</p><p>In order to identify the edges of secondary objects, this module performs two tasks:  <ol> <li>Finds the dividing lines between secondary objects which touch each other.</li>  <li>Finds the dividing lines between the secondary objects and the background of the image. In most cases, this is done by thresholding the image stained for the secondary objects.</li> </ol><p><h4>What do I need as input?</h4> This module identifies secondary objects based on two types of input: <ol> <li>An <i>object</i> (e.g., nuclei) identified from a prior module. These are typically produced by an <b>IdentifyPrimaryObjects</b> module, but any object produced by another module may be selected for this purpose.</li> <li>An <i>image</i> highlighting the image features defining the cell edges. This is typically a fluorescent stain for the cell body, membrane or cytoskeleton (e.g., phalloidin staining for actin).  However, any image which produces these features can be used for this purpose. For example, an image processing module might be used to transform a brightfield image into one which captures the characteristics of a cell  body flourescent stain.</li> </ol><p><h4>What do the settings mean?</h4> See below for help on the individual settings. The following icons are used to call attention to key items: <ul> <li><img src="images\thumb-up.png">&nbsp;Our recommendation or example use case for which a particular setting is best used.</li> <li><img src="images\thumb-down.png">&nbsp;Indicates a condition under which  a particular setting may not work well.</li> <li><img src="images\gear.png">&nbsp;Technical note. Provides more detailed information on the setting, if interested.</li> </ul><p><h4>What do I get as output?</h4> A set of secondary objects are produced by this module, which can be used in downstream modules for measurement purposes or other operations. Because each primary object is used as the starting point  for producing a corresponding secondary object, keep in mind the following points: <ul> <li>The primary object will always be completely contained  within a secondary object. For example, nuclei are completely enclosed within identified cells stained for actin.</li> <li>There will always be at most one secondary object for each primary object.</li> </ul> See the section <a href="#Available_measurements">"Available measurements"</a> below for  the measurements that are produced by this module.<p>Once the module has finished processing, the module display window  will show the following panels: <ul> <li><i>Upper left:</i> The raw, original image.</li> <li><i>Upper right:</i> The identified objects shown as a color image where connected pixels that belong to the same object are assigned the same color (<i>label image</i>). It is important to note that assigned colors are  arbitrary; they are used simply to help you distingush the various objects. </li> <li><i>Lower left:</i> The raw image overlaid with the colored outlines of the  identified secondary objects. The objects are shown with the following colors: <ul> <li>Magenta: Secondary objects</li> <li>Green: Primary objects</li> </ul> If you need to change the color defaults, you can  make adjustments in <i>File > Preferences</i>.</li> <li><i>Lower right:</i> A table showing some of the settings selected by the user, as well as those calculated by the module in order to produce the objects shown.</li> </ul>               <a name="Available_measurements"> <h4>Available measurements</h4> <b>Image measurements:</b> <ul> <li><i>Count:</i> The number of secondary objects identified.</li> <li><i>OriginalThreshold:</i> The global threshold for the image.</li> <li><i>FinalThreshold:</i> For the global threshold methods, this value is the same as <i>OriginalThreshold</i>. For the adaptive or per-object methods, this value is the mean of the local thresholds.</li> <li><i>WeightedVariance:</i> The sum of the log-transformed variances of the  foreground and background pixels, weighted by the number of pixels in  each distribution.</li> <li><i>SumOfEntropies:</i> The sum of entropies computed from the foreground and background distributions.</li> </ul><p><b>Object measurements:</b> <ul> <li><i>Parent:</i> The identity of the primary object associated with each secondary  object.</li> <li><i>Location_X, Location_Y:</i> The pixel (X,Y) coordinates of the center of  mass of the identified secondary objects.</li> </ul><p><h4>Technical notes</h4> The <i>Propagation</i> algorithm is the default approach for secondary object creation,  creating each primary object as a "seed" guided by the input image and limited to the  foreground region as determined by the chosen thresholding method. &lambda; is  a regularization parameter; see the help for the setting for more details. Propagation of secondary object labels is by the shortest path to an adjacent primary object  from the starting ("seeding") primary object. The seed-to-pixel distances are calculated as the sum of absolute differences in a 3x3 (8-connected) image  neighborhood, combined with &lambda; via sqrt(differences<sup>2</sup> + &lambda;<sup>2</sup>).                     <p>See also the other <b>Identify</b> modules.</p> </div><div><h2>Settings:</h2><h4>Select the input objects</h4><div>
            What did you call the objects you want to use as "seeds" to identify a secondary 
            object around each one? By definition, each primary object must be associated with exactly one 
            secondary object and completely contained within it.</div><h4>Name the objects to be identified</h4><div>
            Enter the name that you want to call the objects identified by this module.</div><h4>Select the method to identify the secondary objects</h4><div>
            <p>There are several methods available to find the dividing lines 
            between secondary objects which touch each other:
            <ul>
            <li><i>Propagation:</i> This method will find dividing lines
            between clumped objects where the image stained for secondary objects
            shows a change in staining (i.e., either a dimmer or a brighter line).
            Smoother lines work better, but unlike the Watershed method, small gaps
            are tolerated. This method is considered an improvement on the
            traditional <i>Watershed</i> method. The dividing lines between objects are
            determined by a combination of the distance to the nearest primary object
            and intensity gradients. This algorithm uses local image similarity to
            guide the location of boundaries between cells. Boundaries are
            preferentially placed where the image's local appearance changes
            perpendicularly to the boundary (<i>Jones et al, 2005</i>).</li>
           
            <li><i>Watershed - Gradient:</i> This method uses the watershed algorithm
            (<i>Vincent and Soille, 1991</i>) to assign
            pixels to the primary objects which act as seeds for the watershed.
            In this variant, the watershed algorithm operates on the Sobel
            transformed image which computes an intensity gradient. This method
            works best when the image intensity drops off or increases rapidly
            near the boundary between cells.
            </li>
            <li><i>Watershed - Image:</i> This method is similar to the above,
            but it uses the inverted intensity of the image for the watershed.
            The areas of lowest intensity will form the boundaries between
            cells. This method works best when there is a saddle of relatively
            low intensity at the cell-cell boundary.
            </li>
            <li><i>Distance:</i> In this method, the edges of the primary
            objects are expanded a specified distance to create the secondary
            objects. For example, if nuclei are labeled but there is no stain to help
            locate cell edges, the nuclei can simply be expanded in order to estimate
            the cell's location. This is often called the "doughnut" or "annulus" or
            "ring" approach for identifying the cytoplasm. 
            There are two methods that can be used:
            <ul>
            <li><i>Distance - N</i>: In this method, the image of the secondary 
            staining is not used at all; the expanded objects are the 
            final secondary objects.</li> 
            <li><i>Distance - B</i>: Thresholding of the secondary staining image is used to eliminate background
            regions from the secondary objects. This allows the extent of the
            secondary objects to be limited to a certain distance away from the edge
            of the primary objects without including regions of background.</li></ul></li>
            </ul>
            <b>References</b>
            <ul>
            <li>Jones TR, Carpenter AE, Golland P (2005) "Voronoi-Based Segmentation of Cells on Image Manifolds",
            <i>ICCV Workshop on Computer Vision for Biomedical Image Applications</i>, 535-543. 
            (<a href="http://d1zymp9ayga15t.cloudfront.net/content/papers/JonesCVBIA2005.pdf">link</a>)</li>
            <li>(Vincent L, Soille P (1991) "Watersheds in Digital Spaces: An Efficient Algorithm Based on Immersion
            Simulations", <i>IEEE Transactions of Pattern Analysis and Machine
            Intelligence</i>, 13(6): 583-598 
            (<a href="http://dx.doi.org/10.1109/34.87344">link</a>)</li>
            </ul></div><h4>Select the input image</h4><div>
            The selected image will be used to find the edges of the secondary objects.
            For <i>Distance - N</i> this will not affect object identification, 
            only the final display.</div><h4>Threshold strategy</h4><div>
            The thresholding strategy determines the type of input that is used
            to calculate the threshold. The image thresholds can be based on:
            <ul>
            <li>The pixel intensities of the input image (this is the most common).</li>
            <li>A single value manually provided by the user.</li>
            <li>A single value produced by a prior module measurement.</li>
            <li>A binary image (called a <i>mask</i>) where some of the pixel intensity 
            values are set to 0, and others are set to 1.</li>
            </ul>
            These options allow you to calculate a threshold based on the whole 
            image or based on image sub-regions such as user-defined masks or
            objects supplied by a prior module.
            <br>
            The choices for the threshold strategy are:
            <br><ul>
            <li><i>Automatic:</i> Use the default settings for
            thresholding. This strategy calculates the threshold using the MCT method
            on the whole image (see below for details on this method) and applies the 
            threshold to the image, smoothed with a Gaussian with sigma of 1.
            <dl>
            <dd><img src="images\thumb-up.png">&nbsp;
            This approach is fairly robust, but does not allow you to select the threshold
            algorithm and does not allow you to apply additional corrections to the
            threshold.</dd>
            </dl></li>
            
            <li><i>Global:</i> Calculate a single threshold value based on
            the unmasked pixels of the input image and use that value
            to classify pixels above the threshold as foreground and below
            as background.
            <dl>
            <dd><img src="images\thumb-up.png">&nbsp;
            This strategy is fast and robust, especially if
            the background is uniformly illuminated.</dd>
            </dl></li>
            
            <li><i>Adaptive:</i> Partition the input image into tiles
            and calculate thresholds for each tile. For each tile, the calculated
            threshold is applied only to the pixels within that tile. <br>
            <dl>
            <dd><img src="images\thumb-up.png">&nbsp;
            This method is slower but can produce better results for non-uniform backgrounds.
            However, for signifcant illumination variation, using the <b>CorrectIllumination</b>
            modules is preferable.</dd>
            </dl></li>
            
            <li><i>Per object:</i> Use objects from a prior module
            such as <b>IdentifyPrimaryObjects</b> to define the region of interest
            to be thresholded. Calculate a separate threshold for each object and 
            then apply that threshold to pixels within the object. The pixels outside 
            the objects are classified as background.
            <dl>
            <dd><img src="images\thumb-up.png">&nbsp;
            This method can be useful for identifying sub-cellular particles or 
            single-molecule probes if the background intensity varies from cell to cell
            (e.g., autofluorescence or other mechanisms).</dd>
            </dl></li>
            
            <li><i>Manual:</i> Enter a single value between zero and
            one that applies to all cycles and is independent of the input
            image.
            <dl>
            <dd><img src="images\thumb-up.png">&nbsp;
            This approach is useful if the input image has a stable or
            negligible background, or if the input image is the probability
            map output of the <b>ClassifyPixels</b> module (in which case, a value
            of 0.5 should be chosen). If the input image is already binary (i.e.,
            where the foreground is 1 and the background is 0), a manual value 
            of 0.5 will identify the objects.</dd>
            </dl></li>
            
            <li><i>Binary image:</i> Use a binary image to classify
            pixels as foreground or background. Pixel values other than zero
            will be foreground and pixel values that are zero will be
            background. This method can be used to import a ground-truth segmentation created 
            by CellProfiler or another program. 
            <dl>
            <dd><img src="images\thumb-up.png">&nbsp;
            The most typical approach to produce a 
            binary image is to use the <b>ApplyThreshold</b> module (image as input, 
            image as output) or the <b>ConvertObjectsToImage</b> module (objects as input, 
            image as output); both have options to produce a binary image. It can also be 
            used to create objects from an image mask produced by other CellProfiler 
            modules, such as <b>Morph</b>. Note that even though no algorithm is actually 
            used to find the threshold in this case, the final threshold value is reported 
            as the <i>Otsu</i> threshold calculated for the foreground region.</dd>
            </dl></li>
            
            <li><i>Measurement:</i> Use a prior image measurement as the
            threshold. The measurement should have values between zero and one.
            This strategy can be used to apply a pre-calculated threshold imported 
            as per-image metadata via the <b>Metadata</b> module.
            <dl>
            <dd><img src="images\thumb-up.png">&nbsp;
            Like manual thresholding, this approach can be useful when you are certain what 
            the cutoff should be. The difference in this case is that the desired threshold does 
            vary from image to image in the experiment but can be measured using another module,
            such as one of the <b>Measure</b> modules, <b>ApplyThreshold</b> or
            an <b>Identify</b> module.</dd>
            </dl></li>
            </ul>
	    </div><h4>Regularization factor</h4><div>
            <i>(Used only if Propagation method is selected)</i> <br>
            The regularization factor &lambda; can be anywhere in the range 0 to infinity.
            This method takes two factors into account when deciding where to draw
            the dividing line between two touching secondary objects: the distance to
            the nearest primary object, and the intensity of the secondary object
            image. The regularization factor controls the balance between these two
            considerations: 
            <ul>
            <li>A &lambda; value of 0 means that the distance to the nearest
            primary object is ignored and the decision is made entirely on the
            intensity gradient between the two competing primary objects. </li>
            <li>Larger values of &lambda; put more and more weight on the distance between the two objects.
            This relationship is such that small changes in &lambda; will have fairly different 
            results (e.,g 0.01 vs 0.001). However, the intensity image is almost completely 
            ignored at &lambda; much greater than 1.</li>
            <li>At infinity, the result will look like Distance - B, masked to the
            secondary staining image.</li>
            </ul></div><h4>Fill holes in identified objects?</h4><div>
            Select <i>Yes</i> to fill any holes inside objects.</div><h4>Discard secondary objects touching the border of the image?</h4><div>
            Select <i>Yes</i> to discard secondary objects which touch
            the image border. Select <i>No</i> to retain objects regardless
            of whether they touch the image edge or not.
            <p>The objects are discarded
            with respect to downstream measurement modules, but they are retained in memory
            as "unedited objects"; this allows them to be considered in downstream modules that modify the
            segmentation.</p></div><h4>Discard the associated primary objects?</h4><div>
            <i>(Used only if discarding secondary objects touching the image border)</i> <br>
            It might be appropriate to discard the primary object
            for any secondary object that touches the edge of the image.
            <p>Select <i>Yes</i> to create a new set of objects that are identical 
            to the original primary objects set, minus the objects for which the associated 
            secondary object touches the image edge.</p></div><h4>Name the new primary objects</h4><div>
            <i>(Used only if associated primary objects are discarded)</i> <br>
            You can name the primary objects that remain after the discarding step.
            These objects will all have secondary objects
            that do not touch the edge of the image. Note that any primary object
            whose secondary object touches the edge will be retained in memory as an
            "unedited object"; this allows them to be considered in downstream modules that modify the
            segmentation.</div><h4>Retain outlines of the new primary objects?</h4><div>
            <i>(Used only if associated primary objects are discarded)</i><br>
            Select <i>Yes</i> to retain the outlines of the new objects 
for later use in the pipeline. For example, a common use is for quality control purposes by 
overlaying them on your image of choice using the <b>OverlayOutlines</b> module and then saving 
the overlay image with the <b>SaveImages</b> module.</div><h4>Name the new primary object outlines</h4><div>
            <i>(Used only if associated primary objects are discarded and saving outlines of new primary objects)</i><br>
            Enter a name for the outlines of the identified 
            objects. The outlined image can be selected in downstream modules by selecting 
            them from any drop-down image list.</div><h4>Retain outlines of the identified secondary objects?</h4><div>
            Select <i>Yes</i> to retain the outlines of the new objects 
for later use in the pipeline. For example, a common use is for quality control purposes by 
overlaying them on your image of choice using the <b>OverlayOutlines</b> module and then saving 
the overlay image with the <b>SaveImages</b> module.</div><h4>Name the outline image</h4><div>
            
<i>(Used only if the outline image is to be retained for later use in the pipeline)</i> <br>
Enter a name for the outlines of the identified 
objects. The outlined image can be selected in downstream modules by selecting 
them from any drop-down image list.</div></div>

<div><p><img src="images\IdentifySecondaryObjects.png", width="50%"></p></div>
</body></html>